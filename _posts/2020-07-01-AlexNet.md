---

layout: single
title:  "AlexNet"
header:
  teaser: ""
categories: 

  - Machine Learning
tags:
  - AlexNet
comments: True
---

## AlexNet (2012) [Paper Link](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf)

- ImageNet 데이터셋을 이용하여 Classification 성능을 겨루는 대회인 ILSVRC 2012 대회에서 압도적인 성능으로 우승
저자의 이름 Alex와 Network를 합쳐서
- LeNet과 구조는 비슷 다음과 같은 변화가 있다.
    - 활성화 함수 : ReLU
    - 국소적 정규화(Local Respoinse Normalization)를 실시하는 계층을 이용
    - 드롭아웃 적용
- 구조

    ![Untitled](https://user-images.githubusercontent.com/48716219/90314307-97c57f00-df4d-11ea-96ee-b9aff80fbec2.png)

    - 두 갈래로 나뉨 + 중간 중간 값을 공유
        
        - 2개의 gpu사용했기 때문
    - 224 x 224 크기의 RGB 3 channel image 를 input으로 사용
    - 8개의 층을 사용 : 5개의 Conv layer,  3개의 FC layer

    - 특징
        - 활성화 함수를 ReLU로 사용했다.
            - 기존의  $f(x) = tanh(x)$, $f(x) = (1+e^{-x})^{-1}$ 와 같은 saturating nonlinearities (포화 비선형성..?)은 $f(x)=max(0,x)$ 와 같은 non saturating nonlinearlites보다 느리다

                ![Untitled 1](https://user-images.githubusercontent.com/48716219/90314311-9eec8d00-df4d-11ea-937d-f76e9c390bc4.png)

                실선 ReLU, 점선 tanh

        - 2개의 multiple GPU를 사용
            - 2개의 gpu로 병렬화 기법을 사용
            - 하지만 특정 층에서만 두개의 gpu가 communicate해야함
                - 맨 위에 구조를 보면 두 갈래로 나뉘고 중간 중간 결과를 공유한다. 이는 두개의 gpu가 연산을 수행하면서 중간에 공유하는 것.
        - Local Response Normalization (LRN)
            - 최근에는 거의 사용하지 않는 normalization 테크닉
            - 측면 억제를 사용했다.. (lateral inhibition) → ReLU 때문에
            측면억제? 어떠한 특정 값이 다른 부분에 영향을 끼치는 현상
                - ReLU는 양수의 방향으로 입력값을 그대로 사용
                - 하지만 Conv혹은 Pooling 후 매우 높은 하나의 픽셀값이 주변에 영향을 미칠 것이다.
                - 그러한 현상을 억제하기 위해 사용하는 정규화 방법이 LRN
            - 하지만 최근에는 Batch Normalization을 사용한다고 하니 깊게는 안들어 가겠다.
        - Overlapping Pooling
            - Pooling의 kernel size를 stride보다 크게 하는 방법
            - 일반적으로 pooling을 할 때는 겹치지 않게 하지만 3x3영역을 2픽셀 단위로 pooling하여 조금씩 겹치는 부분이 있도록 pooling하여, overfitting 현상을 개선하였다.

            ![Untitled 2](https://user-images.githubusercontent.com/48716219/90314312-a6ac3180-df4d-11ea-9c3e-d82f07df57cf.png)

- Reduce Overfitting
    - Data Argumentation
        1. Image를 반전시키거나 조금씩 변화시키는 것
        2. RGB의 값을 조금씩 변화를 주는 것 _ PCA활용

            PCA : 주성분 분석.. Principal Component Analysis
            영상인식, 통계 데이터 분석(주성분 찾기), 데이터 압축(차원감소), 노이즈 제거 등 다양한 분야에서 활용

    - Dropout

- 참고링크
    - LRN

    [LRN(Local Response Normalization) 이란 무엇인가?(feat. AlexNet)](https://taeguu.tistory.com/29)

    - PCA _ C. eigenface와 영상인식 응용 부분 참고

    [[선형대수학 #6] 주성분분석(PCA)의 이해와 활용](https://darkpgmr.tistory.com/110)