---

layout: single
title:  "Lecture2, Image Classification_1"
header:
  teaser: ""
categories: 

  - Machine Learning
tags:
  - cs231n

---



# Lecture 2 , Image Classification_1

'이 포스팅의 슬라이드는 cs231n 강의 슬라이드에서 캡쳐했습니다.`

일주일에 한 강의, 강의는 조금씩 나눠서 올릴 계획입니다.

## 자료

[강의영상](https://www.youtube.com/watch?v=OoUX-nOEjG0&t=1750s)

[강의자료](http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture2.pdf)

---

## Image Classification

- **The data-driven approach**
- K-nearest neighbor
- Linear classification 1

---

## The data-driven approach

제목부터 살펴보면? **'데이터 중심적 접근'**이다. 이 제목이 뜻하는 의미가 무엇일까?

### 고양이

Computer Vision에서 가장 중요한 일은 어떠한 사진을 입력을 받게 된다. 이때 컴퓨터는 기존에 미리 주어진 카테고리에서 그 사진이 어느 카테고리에 속하는지 판단하는 것이다. 

컴퓨터는 사진을 읽어 들일떄, 엄청나게 거대한 배열로 사진을 받아들인다.
배열의 각 픽셀은 3개의 값으로 Red, Green, Blue(RGB)값을 나타내고 있고 각각 0부터 255의 값을 갖게 된다.

(이때 0은 흰색, 255는 검은색이다.)

![../assets/images/cs231_2_1/01.png](../assets/images/cs231_2_1/01.png)

![../assets/images/cs231_2_1/02.png](../assets/images/cs231_2_1/02.png)

> 고양이가 숫자로 변했다.

여기서 **Semantic Gap, 의미론적 차이**가 나오게 된다.
[위키백과](https://en.wikipedia.org/wiki/Semantic_gap)에서는 다음과 같이 설명한다.

> 언어 또는 기호와 같은 다른 언어 표현으로 객체에 대한 두 설명 간의 차이를 특성화합니다. Hein에 따르면, 시맨틱 갭은 "다른 표현 시스템 내에 형성된 구성들 간의 의미의 차이"로 정의 될 수 있다.

이처럼 컴퓨터는 거대한 배열로 사진을 받아 들이게 되고, 이때 '고양이'라는 것은 배열에 대한 의미상의 label값이 되는 것이다.

그렇기 때문에 사진이 고양이라는 사실과 컴퓨터가 보는 픽셀값 사이에는 이러한 의미론적 차이가 생긴다.

다음으로는 현재 Computer Vision에서 어려운 부분들을 보여주고 있다.

![../assets/images/cs231_2_1/03.png](../assets/images/cs231_2_1/03.png)

첫 번째로 ***시점***에 관한 문제점이다.

고양이가 앉아있다. 사진을 찍었다. ~~고양이가 과연 가만히 있어줄까?~~
그게 중요한게 아니다.

삼각대에 올려두고 찍지 않는 이상 앵글은 정확하게 고정되지 않을 것이다.
이처럼 미묘한 변화만 주고 사진을 찍어도 배열의 값은 모조리 바뀌게 될것이다!
그뿐만이 아니다.

갑작스럽게 주변에 큰 물체가 지나가면서 그림자가 지거나 고양이의 앞모습을 찍고 뒷모습을 찍는다면?

이러한 상황들에서 각각 찍은 사진들은 ***빛***과 같이 외부적인 요인에 쉽게 영향을 받고 이를 커다란 배열로 확인해본다면 고양이라도 모두 다른 값들이 들어가있을 것이다.

그것이 바로 다음 슬라이드에 나온다.

![../assets/images/cs231_2_1/04.png](../assets/images/cs231_2_1/04.png)

사진들은 모두 고양이이다. 하지만 빛이 어느 방향에서 오는지, 빛의 세기에 따라서 컴퓨터가 보기에는 전혀 고양이라고 알지 못할 것이다. ~~아마도~~

실제로 이와 관련되서 2016년에 '테슬라 모델 S'가 햇빛으로 흰색 식별을 잘못해서 사망사고가 일어난 일도 있었다.

[사람 잡은 자율주행차... 흰색 트럭을 햇빛으로 착각](https://news.chosun.com/site/data/html_dir/2016/07/02/2016070200203.html?utm_source=urlcopy&utm_medium=share&utm_campaign=news)

다음으로 생각해보면 고양이의 ***포즈***이다. 

> [고양이 액체설](https://www.google.com/search?q=%EA%B3%A0%EC%96%91%EC%9D%B4+%EC%95%A1%EC%A0%9C%EC%84%A4&oq=%EA%B3%A0%EC%96%91%EC%9D%B4+%EC%95%A1%EC%A0%9C%EC%84%A4&aqs=chrome..69i57j33.3854j0j7&sourceid=chrome&ie=UTF-8)

![../assets/images/cs231_2_1/05.png](../assets/images/cs231_2_1/05.png)

컴퓨터뿐만 아니라 사람이 봐도 고양이가 맞나 싶을 정도로 고양이는 다양한 포즈로 현생을 살아가고 있다. 
이러한 고양이들까지도 완벽하게 인식을 할 수 있다면, 그것은 정말 강력한 알고리즘일 것이다.

다음 슬라이드를 확인해본다.

![../assets/images/cs231_2_1/06.png](../assets/images/cs231_2_1/06.png)

***가려짐(Occlusion)*** 또한 문제가 될 수 있다. 어딘가에 숨어있거나, 얼굴만 보일 경우... 저런 경우처럼 고양이가 가려져 있는 경우 컴퓨터가 '고양이'를 찾기는 더더욱 힘들어 질 것이다.

그 외에도 고양이가 ***배경과 비슷할 경우***도 있을 것이고

![../assets/images/cs231_2_1/07.png](../assets/images/cs231_2_1/07.png)

![../assets/images/cs231_2_1/08.png](../assets/images/cs231_2_1/08.png)

같은 고양이이지만 고양이마다 '모양', '크기', '색'과 같은 것들이 모두 다를 것이다.

![../assets/images/cs231_2_1/09.png](../assets/images/cs231_2_1/09.png)

위 사진처럼 ***하나의 클래스 안에도 다양성***이 존재한다.
'고양이'라는 하나의 개념으로 이과 같은 다양한 모습들을 전부 소화해내야한다.

이렇게 많은 어려움들을 견디고 '고양이'를 찾는 알고리즘을 만들어야 할 것이다.

사람이라면 물론 쉽게 구분할 수 있지만, 컴퓨터는 이러한 사진들을 숫자들이 가득한 거대한 배열로 받아서 많은 여러움을 극복하고 '고양이'를 찾을 수 있게 된다고 해보자. (물론 이것도 정말 힘든 문제이다.)

할지라도 또 다른 새로운 객체들을 찾아낸다는 것은 정말로 어려운 문제가 될것이다.

하지만 최근에 들어서는 몇가지의 제한사항으로 사람과 맞먹는 속도의 매우 빠른 속도로 사진에서 고양이를 찾아내고, 다른 객체를 찾아내는 것도 가능해졌다.

### 어떻게 했는가?

이제부터 함께 알아보도록 하자.

간단하게 생각해 본다면 다음과 같이 `classify_image` 함수를 표현할 수 있을 것이다.

```python
def classify_image(image):
	# 어떠한 복잡한 과정들이 있다.
	return class_label
```

Input ⇒ Output
사진 ⇒ 무엇을 나타내는 사진인지 (예를 들면 고양이)

여기서 '복잡한 과정들'은 기존에 정렬 알고리즘과는 다르게,
직관적이고 명시적인 안고리즘은 존재하지 않는다.

---

![../assets/images/cs231_2_1/10.png](../assets/images/cs231_2_1/10.png)

![../assets/images/cs231_2_1/11.png](../assets/images/cs231_2_1/11.png)

고양이의 사진을 통해서 'Edge'들을 검출하고
'Edge'들이 만나서 생기는 많은 'Corner'에 대한 정보를 찾아낸다.

이러한 방식으로 모든 정보들을 통해서 고양이를 인식하게 되고 일종의 '규칙 집합'으로 만들 수 있다.

하지만 이런 방식은 문제가 있다.

1. 강인하지 않은 알고리즘
2. 다른 객체를 인식해야 한다면, 다른 객체에 대해서도 별도로 만들어야 한다.
즉, 처음부터 다시 만들어야한다.

한마디로 하면

> 확장성이 없다.

세상에는 다양한 객체들이 존재하고 이를 인식하기 위해선 확장성 있는 알고리즘을 만들어야 한다.

---

### 자. 드디어 데이터 중심적 접근에 대한 내용이 나왔다.

그럼 이제 **확장성이 있는** 분류기를 만들어본다.

1. 이미자와 레이블의 자료들을 모은다 ⇒ 데이터셋 만들기
2. 기계학습을 활용해서 분류기를 훈련시킨다
    - 기계학습 알고리즘은 어떤 식으로든 데이터를 잘 요약해주고 다양한 객체들을 인식할 수 있는 모델을 만들어 준다.
3. 새로운 이미지들로 분류기를 평가한다.
    - 그리고 새로운 사진으로 평가를 해본다면 고양이, 강아지, 등등 다른 객체들을 인식해 낼것이다.

위에서 간단하게 나온 `classify_image` 함수와는 조금 달라졌다.

그렇다면 이번에 만든 분류기 필요한 함수들은 몇 개인가?

.

.

.

2개이다

1. Training(학습) 함수

    이 함수의 입력은 **이미지와 레이블**, 출력은 우리가 원하는 **분류기 모델**이다.

    ```python
    def train(image, labels):
    	# 머신러닝의 복잡한 알고리즘
    	return model
    ```

2. Predict(예측) 함수

    입력은 **모델과 예측할 이미지**, 출력은 **이미지의 예측값**이 될 것이다.

    ```python
    def predict(model, test_images):
    	# 모델을 사용해서 새로운 이미지에서
    	# 알맞은 라벨을 예측한다!
    	return test_labels
    ```

강의에서 다음과 같이 표현했다.

### 기계학습의 key insight

> 분류기는 어떻게 동작하는가?
1. 이미지, 레이블로 학습시킨 모델
2. 모델을 사용해서 이미지의 레이블을 예측

이제 이후에는 신경망, 합성곱신경망, 딥러닝과 같은 내용들이 계속해서 같이 공부할 것이다.

그전에 여기서 잠깐 끊고 

다음에는

간단한 분류기(classifier)를 살펴보도록 할 것이다.

---

---

### 데이터 중심적 접근 (Data-Driven Approach)

이 방식은 아주 일반적인 개념이다.

단순히 기계학습뿐만 아니라 다양한 분야에서도 많이 사용되고 있는 이 개념은 현재 AI가 점점 보편화되면서 중요한 개념이 되는 것 같다.

데이터를 통해서 Insight를 얻고 위기를 예측하며, 기계학습 알고리즘에 학습을 시킬 때도 데이터에 대한 충분한 이해를 바탕으로 전처리를 해야 한다.